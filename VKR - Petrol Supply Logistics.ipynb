{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac213ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import ray.rllib.agents.ppo as ppo\n",
    "import random\n",
    "# import jdc\n",
    "import pandas as pd\n",
    "\n",
    "from gym import Env, spaces\n",
    "import time\n",
    "\n",
    "#get a matrix of distances between locations\n",
    "df = pd.read_csv('dist_vologda_matrix.csv', sep=',')\n",
    "df = df.iloc[:, 1:]\n",
    "\n",
    "#some parameters\n",
    "working_hours = 12.0\n",
    "truck_speed = 60.0\n",
    "\n",
    "df = df / 1000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2009142d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove locations, which cannot be served in 12 hours (driver can't get there and come back in 12 hours)\n",
    "to_remove = ((2 * (df.values[-1,:] / truck_speed) < working_hours) == False).nonzero()[0]\n",
    "df = df.drop(df.index[to_remove])\n",
    "df = df.drop(df.columns[to_remove], axis=1)\n",
    "df = df.reset_index(drop=True)\n",
    "N = len(df.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17acf8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[[0,1,2,3,4,5,55],[0,1,2,3,4,5,55]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74737334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = len(df.values[0])\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "febee365",
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom gym environment for our problem\n",
    "class TransportScape(Env):\n",
    "    def __init__(self, env_config):\n",
    "        self.num_of_trucks = N - 1\n",
    "        self.num_of_locations = N\n",
    "        self.h = working_hours #working hours of each truck driver\n",
    "        self.speed = truck_speed\n",
    "        self.distances = df.values #an array, containing distances between locations\n",
    "    \n",
    "        self.action_space = spaces.MultiDiscrete([self.num_of_trucks, self.num_of_locations - 1])\n",
    "    \n",
    "        self.observation_space = spaces.Dict(\n",
    "        {\n",
    "            #x_i - shows each truck's location\n",
    "            'truck location': spaces.MultiDiscrete(np.full(self.num_of_trucks, self.num_of_locations)),\n",
    "            #p_j - shows whether a truck is assigned to this particular location or not\n",
    "            'assignment': spaces.MultiBinary(self.num_of_locations - 1), \n",
    "            #u_i - shows whether a particular truck was used before\n",
    "            'truck usage': spaces.MultiBinary(self.num_of_trucks),\n",
    "            #delta_i - shows how much time each truck has left until the end of the work day\n",
    "            'time left': spaces.Box(low=np.zeros(self.num_of_trucks), high=np.full(self.num_of_trucks, self.h), dtype=np.float64)\n",
    "        })\n",
    "        self.reset()\n",
    "        \n",
    "    def nice_print(self):\n",
    "        print(\"Truck Location\")\n",
    "        print(self.state['truck location'])\n",
    "        print(\"Assignment\")\n",
    "        print(self.state['assignment'])\n",
    "        print(\"Truck Usage\")\n",
    "        print(self.state['truck usage'])\n",
    "        print(\"Time Left\")\n",
    "        print(self.state['time left'])\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = {'truck location': np.full(self.num_of_trucks, self.num_of_locations - 1),\n",
    "                      'assignment': np.zeros(self.num_of_locations - 1),\n",
    "                      'truck usage': np.zeros(self.num_of_trucks),\n",
    "                      'time left': np.full(self.num_of_trucks, self.h)}\n",
    "        self.done = False\n",
    "        return self.state \n",
    "    \n",
    "    def step(self, action):\n",
    "        i, j = action[0], action[1]\n",
    "        #print(i, j)\n",
    "        truck_loc = self.state['truck location']\n",
    "        assignment = self.state['assignment']\n",
    "        truck_use = self.state['truck usage']\n",
    "        time_left = self.state['time left']\n",
    "        \n",
    "        #if a truck is already assigned to this location - penalty\n",
    "        if assignment[j] == 1:\n",
    "            #print(\"if location was visited\")\n",
    "            self.reward = -5000\n",
    "        #if no time left - penalty\n",
    "        elif (time_left[i] - self.distances[truck_loc[i], j] / self.speed - self.distances[j, -1] / self.speed) < 0:\n",
    "            #print(\"if no time left\")\n",
    "            self.reward = -5000\n",
    "        #else - assign truck to a new location\n",
    "        else:\n",
    "            self.reward = 0\n",
    "            #print(\"else\")\n",
    "            #if needs to use an unused before truck - penalty\n",
    "            if truck_use[i] == 0:\n",
    "                #print(\"new truck requested\")\n",
    "                self.reward = -2000\n",
    "                truck_use[i] = 1\n",
    "            assignment[j] = 1\n",
    "            self.reward -= self.distances[truck_loc[i], j]\n",
    "            time_left[i] -= self.distances[truck_loc[i], j] / self.speed\n",
    "            truck_loc[i] = j\n",
    "        #if all locations have trucks assigned to them - end episode\n",
    "        if np.all(assignment == 1):\n",
    "            #print(\"if all locations are visited\")\n",
    "            self.done = True\n",
    "            \n",
    "        else:\n",
    "            self.done = False\n",
    "        \n",
    "        self.state['truck location'] = truck_loc\n",
    "        self.state['assignment'] = assignment\n",
    "        self.state['truck usage'] = truck_use\n",
    "        self.state['time left'] = time_left\n",
    "        \n",
    "        #self.nice_print()\n",
    "        \n",
    "        # print(self.state, self.reward, self.done)\n",
    "        \n",
    "        return self.state, self.reward, self.done, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f09ab243",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import ray.rllib.agents.ppo as ppo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1c664bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "config[\"num_gpus\"] = 0\n",
    "config[\"num_workers\"] = 1\n",
    "config[\"framework\"] = \"torch\"\n",
    "config[\"env_config\"] = {}\n",
    "config['kl_coeff'] = 0.0\n",
    "config[\"log_level\"] = \"ERROR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "896d2008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RayContext(dashboard_url='', python_version='3.9.11', ray_version='1.12.0', ray_commit='f18fc31c7562990955556899090f8e8656b48d2d', address_info={'node_ip_address': '127.0.0.1', 'raylet_ip_address': '127.0.0.1', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2022-05-04_20-38-29_888144_33722/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2022-05-04_20-38-29_888144_33722/sockets/raylet', 'webui_url': '', 'session_dir': '/tmp/ray/session_2022-05-04_20-38-29_888144_33722', 'metrics_export_port': 63209, 'gcs_address': '127.0.0.1:59400', 'address': '127.0.0.1:59400', 'node_id': '4944a2d940a2a8c388bb89e2df067be3bf9fd708b750de12f29302b0'})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.shutdown()\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d7f55bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-04 20:38:35,046\tINFO ppo.py:268 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "2022-05-04 20:38:35,047\tINFO trainer.py:864 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "2022-05-04 20:38:37,426\tWARNING util.py:60 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "agent = ppo.PPOTrainer(config=config, env=TransportScape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6db012e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:  0\n",
      "mean episode length: 17.932735426008968\n",
      "max episode reward: -9447.045\n",
      "mean episode reward: -69712.29721569506\n",
      "min episode reward: -376280.8533\n",
      "total episodes: 223\n",
      "i:  10\n",
      "mean episode length: 6.777966101694915\n",
      "max episode reward: -7021.0893\n",
      "mean episode reward: -14650.28132677966\n",
      "min episode reward: -54593.1216\n",
      "total episodes: 3975\n",
      "i:  20\n",
      "mean episode length: 6.072948328267477\n",
      "max episode reward: -6938.378699999999\n",
      "mean episode reward: -10295.9296056231\n",
      "min episode reward: -39239.7306\n",
      "total episodes: 10445\n"
     ]
    }
   ],
   "source": [
    "for i in range(21):\n",
    "    # Perform one iteration of training the policy with PPO\n",
    "    result = agent.train()\n",
    "    if i % 10 == 0:\n",
    "        print('i: ', i)\n",
    "        print('mean episode length:', result['episode_len_mean'])\n",
    "        print('max episode reward:', result['episode_reward_max'])\n",
    "        print('mean episode reward:', result['episode_reward_mean'])\n",
    "        print('min episode reward:', result['episode_reward_min'])\n",
    "        print('total episodes:', result['episodes_total'])\n",
    "        checkpoint = agent.save()\n",
    "        #print(\"checkpoint saved at\", checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53e3cb47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state = {'truck location': array([6, 6, 6, 6, 6, 6]), 'assignment': array([0., 0., 0., 0., 0., 0.]), 'truck usage': array([0., 0., 0., 0., 0., 0.]), 'time left': array([12., 12., 12., 12., 12., 12.])} action = [3 4] reward = 0\n",
      "state = {'truck location': array([6, 6, 6, 4, 6, 6]), 'assignment': array([0., 0., 0., 0., 1., 0.]), 'truck usage': array([0., 0., 0., 1., 0., 0.]), 'time left': array([12.        , 12.        , 12.        , 11.26389667, 12.        ,\n",
      "       12.        ])} action = [2 3] reward = -2044.1662\n",
      "state = {'truck location': array([6, 6, 3, 4, 6, 6]), 'assignment': array([0., 0., 0., 1., 1., 0.]), 'truck usage': array([0., 0., 1., 1., 0., 0.]), 'time left': array([12.        , 12.        , 11.22030333, 11.26389667, 12.        ,\n",
      "       12.        ])} action = [2 2] reward = -2046.7818\n",
      "state = {'truck location': array([6, 6, 2, 4, 6, 6]), 'assignment': array([0., 0., 1., 1., 1., 0.]), 'truck usage': array([0., 0., 1., 1., 0., 0.]), 'time left': array([12.        , 12.        ,  5.39117167, 11.26389667, 12.        ,\n",
      "       12.        ])} action = [0 5] reward = -349.7479\n",
      "state = {'truck location': array([5, 6, 2, 4, 6, 6]), 'assignment': array([0., 0., 1., 1., 1., 1.]), 'truck usage': array([1., 0., 1., 1., 0., 0.]), 'time left': array([ 8.47631833, 12.        ,  5.39117167, 11.26389667, 12.        ,\n",
      "       12.        ])} action = [3 1] reward = -2211.4209\n",
      "state = {'truck location': array([5, 6, 2, 1, 6, 6]), 'assignment': array([0., 1., 1., 1., 1., 1.]), 'truck usage': array([1., 0., 1., 1., 0., 0.]), 'time left': array([ 8.47631833, 12.        ,  5.39117167,  5.36595667, 12.        ,\n",
      "       12.        ])} action = [1 0] reward = -353.87640000000005\n",
      "-9320.1986\n"
     ]
    }
   ],
   "source": [
    "env = TransportScape(config)\n",
    "state = env.reset()\n",
    "g = 0\n",
    "done = False\n",
    "reward = 0\n",
    "while not done:\n",
    "  action = agent.compute_action(state, explore = False)\n",
    "  print(f\"state = {state} action = {action} reward = {reward}\")\n",
    "  state, reward, done, info = env.step(action)\n",
    "  g += reward\n",
    "print(g) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec33340c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
